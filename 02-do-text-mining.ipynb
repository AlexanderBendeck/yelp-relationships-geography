{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4842f8e7-7aa5-4878-b31e-eb3fe298c203",
   "metadata": {},
   "source": [
    "# Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42261544-9cf9-459b-9c7a-2e7ad7929c59",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb745d3b-688a-40db-9e14-b93ca2162d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages to use later\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c151a2-e67f-45f4-8bed-de4ad750a63d",
   "metadata": {},
   "source": [
    "## Load & Process Relationship Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "496706e8-2533-4890-a420-c7756a0a5e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['child,children,kid,kids', 'daughter,daughters', 'son,sons', 'parent,parents', 'mother,mom', 'father,dad', 'brother,brothers', 'sister,sisters', 'siblings', 'aunt,aunts', 'uncle,uncles', 'niece,nieces', 'nephew,nephews', 'cousin,cousins', 'grandchild,grandchildren', 'grandmother,grandma', 'grandfather,grandpa', 'grandparents', 'spouse', 'partner', 'husband', 'wife', 'bff', 'relationship', 'date', 'boo,bae,sweetheart', 'fiancee,fiance', 'girlfriend,gf', 'boyfriend,bf', 'friend,friends,buddy,buddies,pal,pals', 'housemate,housemates,roommate,roommates,flatmate,flatmates', 'neighbor,neighbors', 'classmate,classmates', 'professor,professors', 'teacher,teachers', 'coworker,coworkers,colleague,colleagues', 'client,clients', 'boss']\n",
      "{'my_grandmother', 'my_niece', 'my_boyfriend', 'my_children', 'my_buddy', 'my_boo', 'my_date', 'my_daughter', 'my_fiancee', 'my_sisters', 'my_nieces', 'my_sister', 'my_friends', 'my_daughters', 'my_mother', 'my_roommates', 'our_child', 'my_grandchildren', 'my_siblings', 'my_client', 'my_grandma', 'my_grandchild', 'my_clients', 'my_relationship', 'my_flatmates', 'my_brothers', 'my_friend', 'my_uncles', 'my_wife', 'my_dad', 'my_neighbors', 'my_housemate', 'my_partner', 'my_flatmate', 'my_kids', 'my_pals', 'our_son', 'my_boss', 'my_nephews', 'my_bff', 'my_parents', 'my_coworkers', 'my_mom', 'my_colleagues', 'my_grandfather', 'my_spouse', 'my_colleague', 'my_uncle', 'my_housemates', 'my_aunts', 'my_girlfriend', 'my_pal', 'my_gf', 'our_daughters', 'my_parent', 'my_cousin', 'my_bae', 'my_professor', 'my_coworker', 'my_child', 'my_teacher', 'my_teachers', 'my_cousins', 'my_aunt', 'our_daughter', 'my_fiance', 'my_father', 'our_children', 'our_kid', 'my_sons', 'our_sons', 'my_neighbor', 'my_kid', 'my_nephew', 'my_grandparents', 'my_classmates', 'my_grandpa', 'my_brother', 'my_classmate', 'my_son', 'my_professors', 'my_bf', 'my_buddies', 'my_husband', 'my_sweetheart', 'my_roommate', 'our_kids'}\n"
     ]
    }
   ],
   "source": [
    "# Why should we set up the relationships data like this?\n",
    "# 1) Prevents need for stemming each word in each review (time-consuming)\n",
    "# 2) Allows arbitrary groupings of relationships beyond stemming equivalence\n",
    "#    (e.g. \"roommate\" and \"housemate\", \"children\" and \"kids\", etc.)\n",
    "\n",
    "with open(\"relationships/Relationships_ATUS_custom_v2.txt\") as f:\n",
    "    relationships = [line.strip().lower() for line in f.readlines()]\n",
    "\n",
    "relationships_dict = dict()  ## From relationship category to all relevant relationship words, e.g. \"spouse\" --> [\"spouse\", \"partner\"]\n",
    "relationships_dict_reverse = dict()  ## From any relationship word to its category, e.g. \"partner\" --> \"spouse\"\n",
    "\n",
    "for line in relationships:\n",
    "    relevant_words = line.split(\",\")\n",
    "    category = relevant_words[0]\n",
    "    relationships_dict[category] = relevant_words.copy()\n",
    "    for word in relevant_words:\n",
    "        relationships_dict_reverse[word] = category\n",
    "\n",
    "full_relationship_set = set()\n",
    "for relationship_list in relationships_dict.values():\n",
    "    full_relationship_set.update(relationship_list)\n",
    "        \n",
    "print(relationships)\n",
    "#print()\n",
    "#print(relationships_dict)\n",
    "#print()\n",
    "#print(relationships_dict_reverse)\n",
    "\n",
    "words_need_our = [\"child\", \"children\", \"kid\", \"kids\", \"son\", \"sons\", \"daughter\", \"daughters\"]\n",
    "\n",
    "full_relationship_set_new = set()\n",
    "for word in full_relationship_set:\n",
    "    full_relationship_set_new.add(\"my_\" + word)\n",
    "    if word in words_need_our:\n",
    "        full_relationship_set_new.add(\"our_\" + word)\n",
    "print(full_relationship_set_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f0557-bdf0-4157-a114-103d81e54f29",
   "metadata": {},
   "source": [
    "## Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ccfe00f-583f-4c30-8f76-a4cae42e8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metros = ['Boston', 'Portland', 'Austin', 'Orlando', 'Atlanta', 'Vancouver', 'Columbus', 'Boulder']\n",
    "\n",
    "metro = 'Vancouver'\n",
    "# metro_reviews = pd.read_csv(\"small_reviews/yelp_academic_dataset_reviews_\" + metro + \".csv\")\n",
    "metro_reviews = pd.read_csv(\"small_reviews_urbcomp/yelp_academic_dataset_reviews_\" + metro + \".csv\")\n",
    "#metro_reviews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354d2a4b-55f2-4ef2-a409-a7ae9f62017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.25587010383606 sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Clean review text by making everything lowercase\n",
    "metro_reviews[\"text_clean\"] = metro_reviews[\"text\"].str.lower()\n",
    "\n",
    "metro_reviews[\"text_clean\"] = metro_reviews[\"text_clean\"].str.replace(\"my \", \"my_\")\n",
    "metro_reviews[\"text_clean\"] = metro_reviews[\"text_clean\"].str.replace(\"our \", \"our_\")\n",
    "\n",
    "#print(metro_reviews[metro_reviews[\"review_id\"] == \"k9vlSSUStwY2DcjM8Rinnw\"].iloc[0, -1])\n",
    "\n",
    "# Apply tokenizer and get rid of punctuation\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "metro_reviews[\"text_clean\"] = metro_reviews[\"text_clean\"].fillna(\"0\")\n",
    "metro_reviews[\"text_clean\"] = metro_reviews[\"text_clean\"].apply(tokenizer.tokenize)\n",
    "\n",
    "# Remove duplicate words in each review\n",
    "metro_reviews[\"text_clean\"] = metro_reviews[\"text_clean\"].apply(set)\n",
    "\n",
    "# Join tokens with spaces into strings for easy word counting\n",
    "metro_reviews[\"text_clean\"] = metro_reviews[\"text_clean\"].apply(\" \".join)\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1-t0, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "721ad0a3-f7eb-467b-8321-aab814100df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>funny</th>\n",
       "      <th>useful</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>datetime</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>LBJJWJ0uNlIybMfPnQGS0A</td>\n",
       "      <td>I loved everything about this place. I've only...</td>\n",
       "      <td>6Hm2FmfLcU_M91TrZI5htA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2014-02-05 21:09:05</td>\n",
       "      <td>bUHweiErUJ36WGeNrPmEbA</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-02-05 21:09:05</td>\n",
       "      <td>time an though siouxsie went were get service ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ucFOnqgaV40oQ2YNyz5ddQ</td>\n",
       "      <td>Great coffee and pastries. Baristas are excell...</td>\n",
       "      <td>KXCXaF5qimmtKKqnPc_LQA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-03-03 23:45:25</td>\n",
       "      <td>JHXQEayrDHOWGexs0dCviA</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-03 23:45:25</td>\n",
       "      <td>and pastries are vacant the great all sooooo b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1OsubwE6zKNU1fkBGxOFrQ</td>\n",
       "      <td>Ordered the original tonkotsu base ramen and a...</td>\n",
       "      <td>jMz_y_-cWMfiZF7Q5snE6Q</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-01-30 02:39:22</td>\n",
       "      <td>vFnYYmtVwcMGyyGesNImVQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-30 02:39:22</td>\n",
       "      <td>bowl ppl high exit soup old outside my_next ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  funny  useful               review_id  \\\n",
       "0          16      3       5  LBJJWJ0uNlIybMfPnQGS0A   \n",
       "1          17      0       0  ucFOnqgaV40oQ2YNyz5ddQ   \n",
       "2          22      0       0  1OsubwE6zKNU1fkBGxOFrQ   \n",
       "\n",
       "                                                text             business_id  \\\n",
       "0  I loved everything about this place. I've only...  6Hm2FmfLcU_M91TrZI5htA   \n",
       "1  Great coffee and pastries. Baristas are excell...  KXCXaF5qimmtKKqnPc_LQA   \n",
       "2  Ordered the original tonkotsu base ramen and a...  jMz_y_-cWMfiZF7Q5snE6Q   \n",
       "\n",
       "   stars                 date                 user_id  cool  \\\n",
       "0    5.0  2014-02-05 21:09:05  bUHweiErUJ36WGeNrPmEbA     3   \n",
       "1    1.0  2018-03-03 23:45:25  JHXQEayrDHOWGexs0dCviA     0   \n",
       "2    5.0  2016-01-30 02:39:22  vFnYYmtVwcMGyyGesNImVQ     0   \n",
       "\n",
       "              datetime                                         text_clean  \n",
       "0  2014-02-05 21:09:05  time an though siouxsie went were get service ...  \n",
       "1  2018-03-03 23:45:25  and pastries are vacant the great all sooooo b...  \n",
       "2  2016-01-30 02:39:22  bowl ppl high exit soup old outside my_next ca...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metro_reviews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9840c6d1-081b-4fb3-8c56-525259962066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475.2689027786255 sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "relationship_categories = sorted(relationships_dict.keys())\n",
    "relationship_categories_ungroup = sorted(relationships_dict_reverse.keys())\n",
    "\n",
    "df_rows = []\n",
    "df_rows_ungroup = []\n",
    "\n",
    "for i, business_id in enumerate(metro_reviews.business_id.unique()):\n",
    "    reviews_subset = metro_reviews[metro_reviews[\"business_id\"] == business_id]\n",
    "    reviews_subset_counts = reviews_subset.text_clean.str.split().explode().value_counts().reset_index()\n",
    "    x = reviews_subset_counts[reviews_subset_counts[\"index\"].isin(full_relationship_set_new)]\n",
    "#     print(x)\n",
    "#     break\n",
    "    df_row = [business_id, len(reviews_subset)] + [0 for key in relationship_categories]\n",
    "    df_row_ungroup = [business_id, len(reviews_subset)] + [0 for key in relationship_categories_ungroup]\n",
    "\n",
    "    for row in x.itertuples():\n",
    "        key = row.index.split(\"_\")[-1]\n",
    "        \n",
    "        df_row_idx = 2 + relationship_categories.index(relationships_dict_reverse[key])\n",
    "        df_row[df_row_idx] += row.text_clean\n",
    "        \n",
    "        df_row_idx_ungroup = 2 + relationship_categories_ungroup.index(key)\n",
    "        df_row_ungroup[df_row_idx_ungroup] += row.text_clean\n",
    "        \n",
    "    df_rows.append(df_row)\n",
    "    df_rows_ungroup.append(df_row_ungroup)\n",
    "\n",
    "relationship_df = pd.DataFrame(df_rows,columns = [\"business_id\", \"num_reviews\"] + relationship_categories)\n",
    "relationship_df['num_relationship_words'] = relationship_df[relationship_categories].sum(axis=1)\n",
    "relationship_df = relationship_df[[\"business_id\", \"num_reviews\", \"num_relationship_words\"] + relationship_categories]\n",
    "\n",
    "relationship_df_ungroup = pd.DataFrame(df_rows_ungroup,columns = [\"business_id\", \"num_reviews\"] + relationship_categories_ungroup)\n",
    "relationship_df_ungroup['num_relationship_words'] = relationship_df_ungroup[relationship_categories_ungroup].sum(axis=1)\n",
    "relationship_df_ungroup = relationship_df_ungroup[[\"business_id\", \"num_reviews\", \"num_relationship_words\"] + relationship_categories_ungroup]\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1-t0, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d8c5d18-7e8b-4fd9-b640-0f27cf4916fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_df.to_csv(\"output_urbcomp/\" + metro + \"_counts_v2.csv\", index=False)\n",
    "relationship_df_ungroup.to_csv(\"output_urbcomp/\" + metro + \"_counts_ungrouped_v2.csv\", index=False)\n",
    "# relationship_df.to_csv(\"output/\" + metro + \"_counts_v2.csv\", index=False)\n",
    "# relationship_df_ungroup.to_csv(\"output/\" + metro + \"_counts_ungrouped_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738c8fea-883e-415a-b739-aa279a6523ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
